name: DDPM_4GPU
seed: 1234

# VQ-GAN checkpoint (REQUIRED - from trained VQ-GAN)
vqgan_ckpt: ./outputs/vqgan_patches_4gpu/checkpoints/last.ckpt
vae_ckpt: null

# Latent space dimensions (derived from VQ-GAN with downsample [4,4,4])
# Patch: 128×128×128 → Latent: 32×32×32
diffusion_img_size: 32       # H, W after VQ-GAN encoding (128/4 = 32)
diffusion_depth_size: 32     # D after VQ-GAN encoding (128/4 = 32)
diffusion_num_channels: 64   # VQ-GAN embedding_dim
dim_mults: [1, 2, 4, 8]      # UNet channel multipliers

# Training configuration
batch_size: 1   # Per GPU (effective = 4)
num_workers: 12  # Increased from 4 to utilize 96 vCPUs (12 workers × 4 GPUs = 48 total)
gpus: 4
gradient_checkpointing: true  # Enable to reduce memory usage

# Output directory
results_folder: ./checkpoints/ddpm_4gpu/
results_folder_postfix: ''
load_milestone: false

# Conditioning
cond_dim: 512  # X-ray feature dimension (MedCLIP output)
classifier_free_guidance: false  # Disabled to avoid dimension mismatch with labels
medclip: true  # Use MedCLIP for X-ray encoding

# Training hyperparameters
train_lr: 1e-4
timesteps: 1000  # Diffusion steps
sampling_timesteps: 250  # DDIM sampling (faster)
max_epochs: 30  # Total training epochs
train_num_steps: 100000  # Fallback for step-based training
gradient_accumulate_every: 1  # No accumulation needed with 4 GPUs
ema_decay: 0.995
amp: true  # Mixed precision
max_grad_norm: 1.0

# Model architecture
denoising_fn: Unet3D
objective: pred_x0

# Loss weights
loss_type: l1  # Start with simple L1, then enable l1_lpips after warmup
l1_weight: 1.0
perceptual_weight: 0.0  # Disabled initially - unstable with AMP, enable after 5-10 epochs
discriminator_weight: 0.0  # Disabled initially - enable after warmup
classification_weight: 0.0

# LoRA fine-tuning
lora: true
lora_first: false

# Sampling
save_and_sample_every: 1000
num_sample_rows: 1

# Dataset normalization (CTPA statistics)
name_dataset: CTPA
dataset_min_value: -12.911299
dataset_max_value: 9.596558

# Logging
logger: tensorboard

# Distributed settings
find_unused_parameters: true  # Required when auxiliary networks (discriminator/perceptual) are loaded but unused
sync_batchnorm: true
